December 6,2006 - 
i Problem number / Maximum / Score / Grader / 
i 
/ Total /r/i 
There are 10 pages in this quiz, not including this 
one. 
MIT EECS 
6.034: Artificial Intelligence 
(Fall 2006) 
Quiz 4: SVMs and Boosting 
Problem 1: Boosting Identification Trees using ADABOOST 
(50 points) 
As you recall, back in 2001, you were just appointed as a research assistant at PacMan 
Institute of Technology, where you worked for Pinky, Winky, Binky, and Hinky on research 
projects. 
Binky asked you to build a Ghostbot that can collect and dispose of styrofoam foodtruck 
containers (a very useful task around P.I.T.). They should go into two different trucks P 
(for Powerpellet Kitchen) or S (for Strawberry King). 
You were given the following training set (repeated on a tear off sheet for your convenience 
at the end of this examination). Note that "Maybe" is a possible result for the Unfinished? 
test. 
Training Ex. # Colorful? (C) Unfinished? (U) Greasy? (G) Truck (output y) 
1 Y Y N 
s (-1) 
2 N Y N 
s (-1) 
3 N Y N 
s (-1) 
4 Y Y Y 
p (+I) 
5 N N Y 
s (-1) 
G N Maybe Y 
p (+I) 
That time, you completed the project by learning the following ID tree from the training 
data. 
Unfinished? 
S 
Applying ADABOOST 
After sperldillg five years at P.I.T., you are significa,rlt,ly smarter now (or so you hope:) As 
a matter of fact, you have just, recent,ly learned about a great algorithm for learning strong 
Page 1 of 10 
Quiz 4: SVMs and Boosting 
classifiers from data called ADABOOST.You are then reminded that the outcome of that old 
project back in 2001 has not had the success you expected. So you decide to give ADABOOST 
a run at that old data. 
Single-Test Classifiers (aka Decision Stumps) 
First you define the test function as 
+1, if condition holds, 
test (condition) = 
-1, otherwise. 
Mapping Problem Inputs and Outputs 
Then, you map the feature variables into the vector 
x = (Colorful?, Unfinished?, Greasy?). 
You also map Truck output in to t,he output variable 
y = test(Truck = P). 
In other words, if Truck = P, the output is +1 and if Truck = S, the output is -1. 
The Weak Learner 
You then go ahead and program a simple weak learning algorithm that considers only the 
following simple classifiers based on the following 12 tests (6 basics plus their inverses): 
Test I condition 11 Test 
U=N 
Unfinished? = N U#N 
Unfinished? # N 
U=M Unfinished? = Maybe U#M Unfinished? # Maybe I 
G=Y Greasv? = Y G=N Greasv? = N 
Note that Test TRUE (i.e., Always true) means the output of classifier TRUE is always +1, 
or in other words that the value of Truck is always P. Similarly, Test FALSE (i.e., Always 
false) means the output of classifier FALSE is always -1, or in other words that the value 
of Truck is always S. 
If multiple tests have the same weighted error, the learner breaks ties by the test 
label, in order, first from TRUE to G=Y and then from FALSE to G=N, so that 
TRUE is the most preferred and G=N is the least preferred. In other words, 
the order of preference is TRUE, C=Y, U=Y, U=N, U=M, G=Y, FALSE, C=N, 
UfY, U#N, UfM, G=N. 
Page 2 of 10 
Quiz 4: SVMs and Boosting 
Part 1.A: Only Effective Weak/Base Classifiers Needed (10 points) 
Fill in the table below. (Some of the entries have been filled in for you already.) 
Test Misclassified training examples Test Misclassified training examples 
TRUE 
1, 2,3, 5 
FALSE 
4,6 
C=Y 
I,b 
C=N 
2,3,L-1,5 
U=Y 
I,t,3,b 
U#Y 
Y! 5 
U=N 
V!5& 
U#N 
1, 2, 3 
U=M 
U#M 
I,z,~,56 
G=Y 
5 
G=N 
1?2,3,~1,b
-. 
Which tests will never be used? (Cross out all that apply.) 
All w6used 
Page 3 of 10 
- 
- 
Quiz 4: SVMs and Boosting 
Part l.B: Running and Tracking ADABOOST(20 points) 
You decide to run ADABOOSTfor T = 3 rounds, using your simple weak learning algorithm. 
Track the execution of ADABOOSTby filling in the table below the respective values you 
found on each round t = 1,2,3. 
Training # i 
01(2) D2(i) 03(2) 

1 
116 
1/10 
-
I 

18 

2 
116 
/ 
I 
-
I 

10 
18 

3 	
116 
-
L -
I 
10 	 18 
4 	
116 
5/18
1-

3 
5 	
116 
I 
I 
lo 	
3 
6 	
1/6 
L 
IC? 
10 	
18 
Weak classifier ht 	
hl = 
h2= Test h3 = Test 

Test U=M 

G --Y 	
c=Y 
Error c, 	
el = ?2 = 
e3 = 119 
\ 	 \ 
_CC_-
b 
10 

Weights a: 
a: = a; = 
a; = 

I 	 I h5 I b9 I I 
Part 1.C: The 	ADABOOSTClassifier (10 points) 
The final classifier output by ADABOOSTis 
Page 4 of 10 
Quiz 4: SVMs and Boosting 
Part 1.D: Comparing the ID-Tree and ADABOOSTClassifiers on the 
Training Data (5 points) 
Is your new ADABOOSTclassifier worse than your old ID tree classifier from 2001 on the 
training data? (i.e., does your new ADABOOSTclassifier make more mistakes than 
your old ID tree classifier from 2001 on the training data?) 
Yes 
Part l.E: Comparing the ID-Tree and ADABOOSTClassifiers on New 
Unseen Examples (5 points) 
1s your new ADABOOSTclassifier the same as your old ID tree classifier from 2001? (i.e., 
do both classifiers agree on every possible input x?) 
Yes 0 

Page 5 of 10 
Quiz 4: SVMs and Boosting 
Problem 2: Support Vector Machines (50 points) 
Part 2.A: Working with Linear SVMs (14 points) 
Consider the following data set with one positive example Zl= (0, O), yl = +1 and one 
negative example T2= (4,4), y2 = -1. 
Part 2.A.1. Is the data linearly separable? 
No 
Part 2.A.2. 
why not? 
If your answer to Part 2.A.l is No, then explain in a single short sentence 
Page 6 of 10 
Quiz 4: SVMs and Boosting 
If your answer to Part 2.A.1 is Yes, then 
1. provide the SVM classifier below. 

+I, if -( XI+ -\ ~ 2 + 20,

h(2)= 
-1, otherwise. 
2. 	Also provide the weight value of the support vectors 01 and CY~,as well as the offset- 
threshold value b of the SVM classifier. 
-
1 
Ql= IL 
Part 2.B: On Properties of Linear SVMs I (10 points) 
Suppose we have an additional positive example Z3= (- 1,-I), y3 = +1. 
Part 2.B.l. Which data points are support vectors? (Circle) 
none 3a @ 
Page 7 of 10 
Quiz 4: SVMs and Boosting 
Part 2.B.2. If both this data set and that in Part 2.A are linearly separable, then 
1. is the decision boundary for this data set different than for Part 2.A? 
Yes @ 
2. Relative to the support vectors for the data set in Part 2.A, the weights (avalues) 
of the support vectors for the data set in this part are 
the same 	 larger.
0
smaller 
Part 2.C: On Properties of Linear SVMs I1 (10 points) 
Suppose we have an additional positive example 111.4 = (2,2),y4 = tl. 
Part 2.C.1. Which data points are 
none 
Part 2.C.2. If both this data set and t,hat in Part 2.B are linearly separable, then 
1. is the decision boundary for this data set different than for Part 2.B? 

@ No 

2. 	Relative to bhe support vectors for the data set in Part 2.B, the weights (avalues) of 
the slipport vectors for the data set in this part are 
the same smaller 
Page 8 of 10 
Quiz 4: SVMs and Boosting 
Part 2.D: On the Power of SVMs (10 points) 

Suppose we have an additional negative example z5= (-3, -3), y5 = -1. 

-5 -4 -3 -2 -1 0 1 2 3 4 5 
I, 

Circle the number(s) to the left side of the kernels described in the enumerated list below 
that can separate the data. 
1. linear: K(G,G) = ii - v 
@olynomial of degree n 2 2: K(G,ZI) = (1+ G .G)" 
@aha1 Basis Function / Gaussian with sufficiently small scale parameter o:K(G,G) = 
--
II~-~II 
4. none 
Quiz 4: SVMs and Boosting 

Part 2.E: On Data Transformations and Kernels (6 points) 

NOTE: It is possible to solve this problem without solving for the a's. 
Hint: Think of the definition of a kernel. 
Suppose we have two additional examples: one positive example x6 = (+I, +I), y6 = +1, 
and one negative example x7 = (-4, -4), y7 = -1. 
-5 -4 -3 -2 -1 0 1 2 3 4 5 
21 
Consider using the following kernel: 
1. Find the SVM classifier: 
+I, if -2z:+ (3 z,r,+ -2 z;+ O XI+ 0 52+ 25 20,
h(z)= 
-1, otherwise. 
2. Which data points are support vectors? (Circle) 

none 1 2 3 @@ 6 

Page 10 of 10 
